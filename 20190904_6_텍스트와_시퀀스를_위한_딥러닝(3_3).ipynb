{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20190904_6.텍스트와 시퀀스를 위한 딥러닝(3/3).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viridis45/Python-data-analysis/blob/master/20190904_6_%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%99%80_%EC%8B%9C%ED%80%80%EC%8A%A4%EB%A5%BC_%EC%9C%84%ED%95%9C_%EB%94%A5%EB%9F%AC%EB%8B%9D(3_3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhi4QpOnAtJL",
        "colab_type": "text"
      },
      "source": [
        "## 양방향 RNN\n",
        " - 자연어 처리에서 흔히 사용됨\n",
        " - 순서, 시간에 민감\n",
        " - 문장을 이해하는데 단어의 중요성은 단어의 위치에 따라 결정되지 않음 유의\n",
        "   \n",
        "  \n",
        "  \n",
        "시퀀스를 뒤집으면 성능이 어떻게 달라지는지 알아봅시다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV5NsEsBBJsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "51a7301e-dc40-44f1-c292-60d33db52fb7"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "max_features = 1000 # 단어수\n",
        "maxlen = 500 # 사용할 텍스트의 길이\n",
        "\n",
        "(xtrain, ytrain), (xtest, ytest) = imdb.load_data( num_words = max_features)\n",
        "\n",
        "xtrain = [x[::-1] for x in xtrain]\n",
        "xtest = [x[::-1] for x in xtest] # 뒤집음\n",
        "\n",
        "xtrain = sequence.pad_sequences(xtrain, maxlen=maxlen)\n",
        "ytest = sequence.pad_sequences(xtest, maxlen=maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128))\n",
        "model.add(layers.LSTM(32))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(xtrain, ytrain, epochs=10, batch_size=128, validation_split=0.2)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0905 01:42:06.953984 139667791185792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 112s 6ms/step - loss: 0.5660 - acc: 0.7128 - val_loss: 0.4633 - val_acc: 0.7884\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 110s 5ms/step - loss: 0.4326 - acc: 0.8161 - val_loss: 0.5535 - val_acc: 0.7374\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 110s 6ms/step - loss: 0.4079 - acc: 0.8282 - val_loss: 0.5421 - val_acc: 0.7718\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 111s 6ms/step - loss: 0.3785 - acc: 0.8437 - val_loss: 0.3816 - val_acc: 0.8360\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 110s 5ms/step - loss: 0.3613 - acc: 0.8536 - val_loss: 0.4087 - val_acc: 0.8208\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 112s 6ms/step - loss: 0.3544 - acc: 0.8565 - val_loss: 0.3734 - val_acc: 0.8352\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 111s 6ms/step - loss: 0.3350 - acc: 0.8650 - val_loss: 0.3852 - val_acc: 0.8388\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 111s 6ms/step - loss: 0.3272 - acc: 0.8668 - val_loss: 0.4364 - val_acc: 0.8466\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 111s 6ms/step - loss: 0.3313 - acc: 0.8722 - val_loss: 0.3890 - val_acc: 0.8228\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 110s 6ms/step - loss: 0.3223 - acc: 0.8699 - val_loss: 0.3686 - val_acc: 0.8396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rakoj76mFnqD",
        "colab_type": "text"
      },
      "source": [
        "정방향과 동일한 성능 == 언어를 이해하는데에 순서가 중요하지만 결정적이지는 않다\n",
        "\n",
        "- 고로 양뱡향 rnn.\n",
        "- 하나씩 뒤집는듯\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUjI8gdyFkeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "9234e64e-9288-4f10-e4a4-b38270a4dbcf"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 32))\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(xtrain, ytrain, epochs=10, batch_size=128, validation_split=0.2)\n",
        "\n",
        "\n",
        "#위를 이전에 보았던 온도데이터에 접목시켜본다"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 154s 8ms/step - loss: 0.6054 - acc: 0.6684 - val_loss: 0.4843 - val_acc: 0.8022\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 149s 7ms/step - loss: 0.4476 - acc: 0.8082 - val_loss: 0.3791 - val_acc: 0.8478\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 147s 7ms/step - loss: 0.4004 - acc: 0.8347 - val_loss: 0.4658 - val_acc: 0.7830\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 147s 7ms/step - loss: 0.3798 - acc: 0.8466 - val_loss: 0.3882 - val_acc: 0.8370\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 148s 7ms/step - loss: 0.3753 - acc: 0.8478 - val_loss: 0.3825 - val_acc: 0.8448\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 148s 7ms/step - loss: 0.3579 - acc: 0.8570 - val_loss: 0.4233 - val_acc: 0.8286\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 148s 7ms/step - loss: 0.3500 - acc: 0.8572 - val_loss: 0.4452 - val_acc: 0.8316\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 147s 7ms/step - loss: 0.3368 - acc: 0.8638 - val_loss: 0.3818 - val_acc: 0.8606\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 147s 7ms/step - loss: 0.3341 - acc: 0.8664 - val_loss: 0.3825 - val_acc: 0.8466\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 148s 7ms/step - loss: 0.3281 - acc: 0.8692 - val_loss: 0.3643 - val_acc: 0.8448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA--OmzeHQni",
        "colab_type": "text"
      },
      "source": [
        "### 막간을 이용한 Embedding 리뷰 한번 더 [1](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/) [2](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n",
        "- dense vector representation계\n",
        "- traditional BOW : uses large space vec where it's mostly filled with zeros -- sparse\n",
        "- Embedding == projection of word into continuous vec space\n",
        "- based on words surrounding the target word\n",
        "- int for each words, initialized with a random weight\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUL_GKsAQEAt",
        "colab_type": "text"
      },
      "source": [
        "### 온도 예측모델의 성능을 향상시키려면\n",
        "1. 스태킹 유닛 수 조절\n",
        "2. RMSprop 학습률 조절\n",
        "3. LSTM 사용\n",
        "4. 유닛 수가 많은 Dense층을 스태킹\n",
        "5. 최종모델을 테스트세트에서 확인. 검증세트에 과대적합하는것을 방지하기 위해.\n",
        "\n",
        "\n",
        "### 리뷰\n",
        "1. 상식 수준의 기준점에서 시작하자\n",
        "2. 간단한 모델에서 먼저 시도\n",
        "3. 시계열이라면 순환층\n",
        "4. dropout and recurrent_dropout\n",
        "5. 스태킹 RNN로 overkill하지는 말자\n",
        "6. 양방향 RNN은 최근 정보가 더 중요한 데이터에는 부적합\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndFmS7h9ROxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--PT5JUDRQED",
        "colab_type": "text"
      },
      "source": [
        "# 컨브넷을 사용한 시퀀스 처리\n",
        "- 인풋의 특성을 뽑아내는 컨브넷(합성곱연산)을 시퀀스 처리에 사용해보자\n",
        "- 1D 컨브넷 :\n",
        "  - 특정 시퀀스에서 RNN 대체. \n",
        "  - 적은 비용\n",
        "  - 오디오 생성, 기계 번역에서 대세\n",
        "  \n",
        "### 1D합성곱 이해하기\n",
        "- 2D합성곱 어떻게 했는지 기억하니 : 이미지텐서에서 2d패치 추출 후 모든 패치에 동일한 변환 적용\n",
        "- 이걸 1d에 적용한다.\n",
        "- 똑같음. 특정 위치의 패턴이 재발하는것을 인식. 윈도우.\n",
        "- subsampling 하는데에 똑같이 평균풀링, 맥스풀링 적용\n",
        "\n",
        "\n",
        "### 1D 합성곱 쌓기\n",
        "- 2D와 같음.\n",
        "1. Conv1D / MaxPooling1D 쌓기\n",
        "2. GlobalAveragePooling1D / GlobalMaxPooling1D / Flatten으로 마무리\n",
        "- 위는 (samples, timesteps, features)를 입력받고 3d\n",
        "- (samples, features) 를 돌려준다 2d\n",
        "3. GlobalAverage'2D'Pooling 은 4d - 2d : (sample, h, w, channels) -- (samples, channels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVzxd2jNAr4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "outputId": "525a8f48-5e00-4e1c-937b-69bd01f2b15d"
      },
      "source": [
        "# did this above but doing it again for muscle memory\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "max_features = 1000\n",
        "max_len = 100\n",
        "\n",
        "print('loading data')\n",
        "(xtrain, ytrain),(xtest, ytest) = imdb.load_data(num_words = max_features)\n",
        "print(len(xtrain), 'training sequence')\n",
        "print(len(xtest), 'test sequence')\n",
        "\n",
        "print('\\nsequence padding (samples x time)')\n",
        "xtrain = sequence.pad_sequences(xtrain, maxlen=max_len)\n",
        "xtest = sequence.pad_sequences(xtest, maxlen=max_len)\n",
        "\n",
        "print('x_train size: ', xtrain.shape)\n",
        "print('x_test size: ', xtest.shape)\n",
        "\n",
        "\n",
        "# now the convnet\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length = max_len))\n",
        "model.add(layers.Conv1D(32, 7, activation = 'relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32,7,activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=1e-4), loss = 'binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model.fit(xtrain, ytrain, epochs = 10, batch_size = 128, validation_split = 0.2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data\n",
            "25000 training sequence\n",
            "25000 test sequence\n",
            "\n",
            "sequence padding (samples x time)\n",
            "x_train size:  (25000, 100)\n",
            "x_test size:  (25000, 100)\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 100, 128)          128000    \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 94, 32)            28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 18, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 12, 32)            7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 163,937\n",
            "Trainable params: 163,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 18s 899us/step - loss: 0.7344 - acc: 0.5394 - val_loss: 0.6799 - val_acc: 0.5702\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 17s 859us/step - loss: 0.6673 - acc: 0.6151 - val_loss: 0.6621 - val_acc: 0.6182\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 17s 857us/step - loss: 0.6381 - acc: 0.6719 - val_loss: 0.6300 - val_acc: 0.6640\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 18s 876us/step - loss: 0.5843 - acc: 0.7230 - val_loss: 0.5650 - val_acc: 0.7158\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 17s 865us/step - loss: 0.5180 - acc: 0.7651 - val_loss: 0.5187 - val_acc: 0.7510\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 17s 866us/step - loss: 0.4771 - acc: 0.7885 - val_loss: 0.5138 - val_acc: 0.7668\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 17s 861us/step - loss: 0.4537 - acc: 0.8021 - val_loss: 0.5186 - val_acc: 0.7740\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 17s 865us/step - loss: 0.4349 - acc: 0.8095 - val_loss: 0.5109 - val_acc: 0.7734\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 17s 864us/step - loss: 0.4206 - acc: 0.8123 - val_loss: 0.5471 - val_acc: 0.7738\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 18s 893us/step - loss: 0.4060 - acc: 0.8109 - val_loss: 0.6275 - val_acc: 0.7480\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDhIlQF0uTZF",
        "colab_type": "text"
      },
      "source": [
        "### CNN과 RNN연결하여 긴 시퀀스를 처리하기\n",
        "- 컨브넷은 인풋을 독립적으로 처리 -- 타임스텝의 순서에 민감하지 않음\n",
        "- conv1d를 RNN의 전처리용으로 사용하여 경량성을 응용\n",
        "    - 긴 시퀀스의 데이터 경량화\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY2xwGiet4O2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}